{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkewWxOhbjFT"
      },
      "source": [
        "## 필요 프레임워크 설치\n",
        "1. Scrapy 설치\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj2NFRxrTdYw"
      },
      "outputs": [],
      "source": [
        "# scrapy 설치\n",
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVT27ZnKraok"
      },
      "source": [
        "## Project 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scrapy Project 생성 후 spiders 폴더에 sample.py 파일 생성\n",
        "!scrapy startproject sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# settings.py 설정\n",
        "\n",
        "# 실행명령어 설정\n",
        "BOT_NAME = \"news\"\n",
        "\n",
        "# 파일 저장방식 설정\n",
        "FEED_FORMAT = \"csv\"\n",
        "\n",
        "# 저장할 파일명 설정\n",
        "FEED_URI = \"조선일보_test.csv\"\n",
        "\n",
        "# 404 오류무시 설정\n",
        "HTTPERROR_ALLOWED_CODES = [404]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# items.py\n",
        "# Spider에서 추출한 데이터를 구조화된 데이터로 변환해줌\n",
        "import scrapy\n",
        "\n",
        "class NewsItem(scrapy.Item):\n",
        "    # 가져올 데이터 설정\n",
        "    # 기사제목\n",
        "    title = scrapy.Field()\n",
        "    # 기자이름\n",
        "    writer = scrapy.Field()\n",
        "    # 작성날짜\n",
        "    published_date = scrapy.Field()\n",
        "    # 본문\n",
        "    content = scrapy.Field()\n",
        "    # 이미지\n",
        "    imgs = scrapy.Field()\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample.py의 class 실행명령어 설정\n",
        "import scrapy\n",
        "import re # 정규표현식 사용\n",
        "\n",
        "class testSpider(scrapy.Spider):\n",
        "    # 실행명령어\n",
        "    # settings.py에서 설정한 BOT_NAME과 같아야함(주의)\n",
        "    name = 'test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 크롤링 과정\n",
        "\n",
        "1. 크롤링할 메인 url 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def start_request(self):\n",
        "    # 헤더 설정\n",
        "    headers= {\n",
        "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
        "        }\n",
        "    \n",
        "    # 크롤링할 메인 url 가져오기(조선일보)\n",
        "    url = f'http://kid.chosun.com/list_kj.html?catid=1&pn={i}'\n",
        "\n",
        "    # 결과를 리턴하고 종료하지 않고 crawl_page 실행\n",
        "    yield scrapy.Request(url=url, callback=self.crawl_page, headers=headers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. 가져온 메인 url에서 크롤링할 url 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crawl_page(self,response):\n",
        "    headers= {\n",
        "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    # 한 페이지에 10개의 기사가 존재\n",
        "    for i in range(1, 11):\n",
        "        # 예외처리(마지막 페이지에는 10개의 기사가 존재하지 않을 수도 있음.)\n",
        "        try:\n",
        "            # 크롤링할 url\n",
        "            # xpath로 크롤링할 url 추출\n",
        "            url = 'http://kid.chosun.com' + response.xpath(f'//*[@id=\"container\"]/section[2]/article/ul/li[{i}]/div[2]/div[1]/a/@href')[0].extract()\n",
        "            # url이 존재하면\n",
        "            if url:\n",
        "                # 종료하지 않고 parse 함수 호출\n",
        "                yield scrapy.Request(url=url, callback=self.parse, headers=headers)\n",
        "        except Exception as e:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. 요청한 데이터를 파싱 후 필요한 데이터만 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse(self, response):\n",
        "    # 아이템 객체 생성\n",
        "    item = NewsItem()\n",
        "\n",
        "    # xpath로 저장할 데이터 추출\n",
        "    # 기사 제목\n",
        "    item['title'] = response.xpath('//*[@id=\"container\"]/section[1]/div[2]/text()')[0].extract()\n",
        "    \n",
        "    # span_tag의 갯수 확인 -> 기자 이름이 없고 날짜만 있는 기사가 있음\n",
        "    len_span_tags = len(response.xpath('//*[@id=\"container\"]/section[1]/div[3]/div[1]/span').extract())\n",
        "    # span_tag가 2개이상이면 기자 이름, 날짜가 존재\n",
        "    if len_span_tags >= 2:\n",
        "        # 기자 이름 \n",
        "        item['writer'] = response.xpath('//*[@id=\"container\"]/section[1]/div[3]/div[1]/span[1]/text()')[0].extract()\n",
        "        # 작성 날짜\n",
        "        item['published_date'] = response.xpath('//*[@id=\"container\"]/section[1]/div[3]/div[1]/span[@class=\"date\"]/text()')[0].extract().replace('\\r\\n', '')[9:]\n",
        "    # 2개이하면 작성 날짜만 존재\n",
        "    else:\n",
        "        # 기자 이름 null\n",
        "        item['writer'] = ''\n",
        "        # 작성 날짜 \n",
        "        item['published_date'] = response.xpath('//*[@id=\"container\"]/section[1]/div[3]/div[1]/span[1]/text()')[0].extract().replace('\\r\\n', '')[9:]\n",
        "\n",
        "    # 본문을 html 형식으로 가져옴\n",
        "    content_div = response.xpath('//*[@id=\"article\"]').get()\n",
        "    # <> 태그를 정규표현식으로 제거\n",
        "    content_regex = re.sub(r'<[^>]*>', '', content_div)\n",
        "    # 본문\n",
        "    item['content'] = content_regex\n",
        "\n",
        "    # 이미지 리스트\n",
        "    img_list = []\n",
        "    # 정규표현식으로 img태그의 src만 찾기\n",
        "    image_regexs = re.findall(r'<img[^>]*src=\"([^\"]+)\"[^>]*>', content_div)\n",
        "    # 반복문으로 src링크를 하나씩 img_list에 저장\n",
        "    for image_regex in image_regexs:\n",
        "        img_list.append(image_regex)\n",
        "    # 이미지\n",
        "    item['imgs'] = img_list\n",
        "\n",
        "    yield item"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
